{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n",
      "Logged in successfully!\n",
      "Logged in successfully!\n",
      "Logged in successfully!\n",
      "Error navigating to 'Community' tab for https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "Error extracting info for https://huggingface.co/google/gemma-2b\n",
      "Error navigating to 'Community' tab for https://huggingface.co/mobiuslabsgmbh/Llama-3.1-8b-instruct_4bitgs64_hqq_calib\n",
      "Error clicking the username link 2 : https://huggingface.co/google/gemma-2-2b-it\n",
      "Error clicking the username link 2 : https://huggingface.co/mobiuslabsgmbh/Llama-3.1-8b-instruct_4bitgs64_hqq_calib\n",
      "Error fetching submission date for https://huggingface.co/papers/2009.03300 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error navigating to 'Community' tab for https://huggingface.co/google/gemma-2b\n",
      "Error clicking the username link 2 : https://huggingface.co/google/gemma-2b\n",
      "Error fetching submission date for https://huggingface.co/papers/1905.07830 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error fetching submission date for https://huggingface.co/papers/1911.11641 for https://huggingface.co/google/gemma-2-9b-it\n",
      "No arxiv tags for the link: https://huggingface.co/mobiuslabsgmbh/Llama-3.1-8b-instruct_4bitgs64_hqq_calib\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2-2b-it\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2b\n",
      "Error extracting info for https://huggingface.co/google/gemma-2-2b\n",
      "Error extracting info for https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "Error fetching submission date for https://huggingface.co/papers/1904.09728 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error extracting info for https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
      "Error navigating to 'Community' tab for https://huggingface.co/google/gemma-2-2b\n",
      "Error navigating to 'Community' tab for https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "Error clicking the username link 2 : https://huggingface.co/google/gemma-2-2b\n",
      "Error clicking the username link 2 : https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "Error navigating to 'Community' tab for https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
      "Error clicking the username link 2 : https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
      "Error fetching submission date for https://huggingface.co/papers/1905.10044 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error fetching submission date for https://huggingface.co/papers/1907.10641 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error fetching submission date for https://huggingface.co/papers/1811.00937 for https://huggingface.co/google/gemma-2-9b-it\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2-2b\n",
      "No arxiv tags for the link: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
      "Error extracting info for https://huggingface.co/Writer/Palmyra-Med-70B\n",
      "Error extracting info for https://huggingface.co/meta-llama/Meta-Llama-3.1-405B\n",
      "Error extracting info for https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
      "Error fetching submission date for https://huggingface.co/papers/1809.02789 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error navigating to 'Community' tab for https://huggingface.co/Writer/Palmyra-Med-70B\n",
      "Error navigating to 'Community' tab for https://huggingface.co/meta-llama/Meta-Llama-3.1-405B\n",
      "Error clicking the username link 2 : https://huggingface.co/Writer/Palmyra-Med-70B\n",
      "Error clicking the username link 2 : https://huggingface.co/meta-llama/Meta-Llama-3.1-405B\n",
      "Error navigating to 'Community' tab for https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
      "Error clicking the username link 2 : https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
      "Error fetching submission date for https://huggingface.co/papers/1911.01547 for https://huggingface.co/google/gemma-2-9b-it\n",
      "Error fetching submission date for https://huggingface.co/papers/1705.03551 for https://huggingface.co/google/gemma-2-9b-it\n",
      "No arxiv tags for the link: https://huggingface.co/Writer/Palmyra-Med-70B\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-405B\n",
      "Error fetching submission date for https://huggingface.co/papers/2107.03374 for https://huggingface.co/google/gemma-2-9b-it\n",
      "No arxiv tags for the link: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2-9b-it\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
      "No arxiv tags for the link: https://huggingface.co/Writer/Palmyra-Med-70B-32K\n",
      "Logged in successfully!\n",
      "Finished processing batch 1 and saved to Outputs\\Text_generation_results_2\\model_info_batch_1.csv\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "Finished processing batch 2 and saved to Outputs\\Text_generation_results_2\\model_info_batch_2.csv\n",
      "No arxiv tags for the link: https://huggingface.co/Zyphra/Zamba2-2.7B\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "username = os.getenv(\"HUGGINGFACE_USERNAME\")\n",
    "password = os.getenv(\"HUGGINGFACE_PASSWORD\")\n",
    "\n",
    "# Import files\n",
    "import Python_scripts.login as login\n",
    "import Python_scripts.extract_info as extract_info\n",
    "import Python_scripts.click_commit_dates as click_commit\n",
    "import Python_scripts.click_username as click_username\n",
    "import Python_scripts.click_arxiv_tags as click_arxiv_tags\n",
    "import Python_scripts.space_apps_info as space_apps_info\n",
    "import Python_scripts.get_submission_date as get_submission_date\n",
    "import Python_scripts.click_community as click_community\n",
    "import Python_scripts.check_404_error as check_404_error\n",
    "import Python_scripts.files_versions_info as files_versions_info\n",
    "importlib.reload(login)\n",
    "importlib.reload(extract_info)\n",
    "importlib.reload(click_commit)\n",
    "importlib.reload(click_username)\n",
    "importlib.reload(click_arxiv_tags)\n",
    "importlib.reload(space_apps_info)\n",
    "importlib.reload(get_submission_date)\n",
    "importlib.reload(click_community)\n",
    "importlib.reload(check_404_error)\n",
    "importlib.reload(files_versions_info)\n",
    "\n",
    "# Load links\n",
    "nlp_links = pd.read_csv(\"Outputs/model_links-NLP.csv\")\n",
    "nlp_links = nlp_links[nlp_links[\"Tag\"] == \"Text Generation\"]\n",
    "nlp_links.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Set up Chrome options (headless mode)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Process a batch of links in a single thread\n",
    "def process_link_batch(links_batch, username, password):\n",
    "    # Set up Chrome driver for the thread (one instance per thread)\n",
    "    service = Service(executable_path='Dependencies//chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Log in to Hugging Face once for the thread\n",
    "    login.login_hugging_face(driver, username, password)\n",
    "\n",
    "    batch_results = []\n",
    "    for _, row in links_batch.iterrows():  # Iterate through DataFrame rows\n",
    "        link = row['Model Link']\n",
    "        \n",
    "        downloads_all_time = has_arxiv = model_card = language_tag = model_name = likes_element = description_names = model_card_word_count = no_space_apps = 'NA'\n",
    "        community_count = discussions_count = discussions_closed_count = pull_request_count = pull_request_closed_count = 'NA'\n",
    "        info_text = name_text = 'NA'\n",
    "        number_of_commits = latest_commit_date = oldest_commit_date = additional_requirements = 'NA'\n",
    "        arxiv_links = number_of_papers = submission_dates_dt = 'NA'\n",
    "        published_dates_dt = readme_commit_dates = no_unique_readme_commits = time_differences = unique_commit_dates = no_unique_commits = 'NA'\n",
    "        error_flag = 'NA'\n",
    "\n",
    "        try:\n",
    "            error_flag = check_404_error.check_404_page(driver, link)\n",
    "            if error_flag == 0:\n",
    "                # Extract info from the link page\n",
    "                extract_info_list = extract_info.extract_info(driver, link)\n",
    "                downloads_per_month = extract_info_list[0]\n",
    "                downloads_all_time = extract_info_list[1]\n",
    "                model_card = extract_info_list[2]\n",
    "                language_tag = extract_info_list[3]\n",
    "                model_name = extract_info_list[4]\n",
    "                likes_element = extract_info_list[5]\n",
    "                description_names = extract_info_list[6]\n",
    "                model_card_word_count = extract_info_list[7]\n",
    "                no_space_apps = extract_info_list[8]\n",
    "\n",
    "                # Extract info after clicking the Community Tab\n",
    "                click_community_list = click_community.click_comm(driver, link)\n",
    "                community_count = click_community_list[0]\n",
    "                discussions_count = click_community_list[1]\n",
    "                discussions_closed_count = click_community_list[2]\n",
    "                pull_request_count = click_community_list[3]\n",
    "                pull_request_closed_count = click_community_list[4]\n",
    "\n",
    "                # Extract info after clicking on the username link\n",
    "                click_username_list = click_username.get_user_info(driver, link)\n",
    "                info_text = click_username_list[0]\n",
    "                name_text = click_username_list[1]\n",
    "\n",
    "                # Extract info after clicking on the commits link\n",
    "                click_commit_list = click_commit.click_files_and_versions(driver, link)\n",
    "                number_of_commits = click_commit_list[0]\n",
    "                latest_commit_date = click_commit_list[1]\n",
    "                oldest_commit_date = click_commit_list[2]\n",
    "                additional_requirements = click_commit_list[3]\n",
    "                unique_commit_dates = click_commit_list[4]\n",
    "                no_unique_commits = len(unique_commit_dates)\n",
    "\n",
    "                # Extract info after clicking on the ARXIV tags\n",
    "                click_arxiv_tags_list = click_arxiv_tags.get_arxiv_links(driver, link)\n",
    "                arxiv_links = click_arxiv_tags_list[0]\n",
    "                number_of_papers = len(arxiv_links)\n",
    "                if number_of_papers == 0 or number_of_papers == 'NA':\n",
    "                    has_arxiv = 0\n",
    "                elif number_of_papers > 0:\n",
    "                    has_arxiv = 1\n",
    "                submission_dates = click_arxiv_tags_list[1]\n",
    "                submission_dates_dt = [datetime.strptime(date, '%d %b %Y') if date != \"N/A\" else \"N/A\" for date in submission_dates]\n",
    "\n",
    "                get_submission_date_list = get_submission_date.get_readme_info(driver, link)\n",
    "                published_dates = get_submission_date_list[1]\n",
    "                published_dates_dt = [datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') if date != \"N/A\" else \"N/A\" for date in published_dates]\n",
    "                readme_commit_dates = get_submission_date_list[2]\n",
    "                no_unique_readme_commits = len(readme_commit_dates)\n",
    "\n",
    "                time_differences = [\n",
    "                    \"N/A\" if (isinstance(pub_date, str) or isinstance(sub_date, str) or pub_date is None or sub_date is None)\n",
    "                    else (pub_date - sub_date).days\n",
    "                    for pub_date, sub_date in zip(published_dates_dt, submission_dates_dt)\n",
    "                ]\n",
    "\n",
    "            # Save the result for this link\n",
    "            batch_results.append({\n",
    "                'Model Link': link,\n",
    "                'Valid Link?': error_flag,\n",
    "                'Model Name': model_name,\n",
    "                'Language of the Model': language_tag,\n",
    "                'Organization Tags': info_text,\n",
    "                'Name of Organization/Individual': name_text,\n",
    "                'Downloads All Time': downloads_all_time,\n",
    "                'Likes': likes_element,\n",
    "                'Community Contributions': community_count,\n",
    "                'Number of Discussions': discussions_count,\n",
    "                'Closed Discussions': discussions_closed_count,\n",
    "                'Number of Pull Requests': pull_request_count,\n",
    "                'Closed Pull Requests': pull_request_closed_count,\n",
    "                'Has Arxiv Tag': has_arxiv,\n",
    "                'Number of Papers': number_of_papers,\n",
    "                'Links to Paper(s)': arxiv_links,\n",
    "                'Publish Dates of Paper(s)': submission_dates_dt,\n",
    "                'Publish Dates of Paper(s) (on HF)': published_dates_dt,\n",
    "                'Time Difference': time_differences,\n",
    "                'Model Card': model_card,\n",
    "                'Model Card Length': model_card_word_count,\n",
    "                'Number of Sections': len(description_names),\n",
    "                'Model Card Section Names': description_names,\n",
    "                'Number of Commits': number_of_commits,\n",
    "                'Latest Commit Date': latest_commit_date,\n",
    "                'Oldest Commit Date': oldest_commit_date,\n",
    "                'Unique Readme Commit Dates': readme_commit_dates,\n",
    "                'Number of Unique Readme Commit Dates': no_unique_readme_commits,\n",
    "                'Unique Commit Dates': unique_commit_dates,\n",
    "                'Number of Unique Commit Dates': no_unique_commits,\n",
    "                'Number of Space Apps': no_space_apps,\n",
    "                'Additional Requirements': additional_requirements\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link {link}: {str(e)}\")\n",
    "        \n",
    "    # Close the driver for this thread\n",
    "    driver.quit()\n",
    "\n",
    "    # Return the results for this batch of links\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "# Divide the DataFrame into batches for each thread\n",
    "def divide_into_batches(nlp_links, num_batches):\n",
    "    batch_size = len(nlp_links) // num_batches\n",
    "    return [nlp_links[i*batch_size:(i+1)*batch_size] for i in range(num_batches)] + [nlp_links[num_batches*batch_size:]]\n",
    "\n",
    "# Parallel processing with ThreadPoolExecutor\n",
    "def process_links_in_parallel(nlp_links, username, password):\n",
    "    results = []\n",
    "    num_workers = 4  # Number of threads (and number of WebDriver instances)\n",
    "    split_number = 2\n",
    "    batch_start_number = 1\n",
    "    nlp_links = nlp_links[0:40]\n",
    "\n",
    "    # Divide links into batches based on number of workers\n",
    "    link_batches = divide_into_batches(nlp_links, num_workers)\n",
    "\n",
    "    # Use ThreadPoolExecutor to process links concurrently\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_link_batch, batch, username, password) for batch in link_batches]\n",
    "        \n",
    "        for index, future in enumerate(as_completed(futures)):\n",
    "            batch_results = future.result()\n",
    "            if batch_results:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # Save results to CSV in batches\n",
    "            if (index + 1) % split_number == 0 or (index + 1) == len(nlp_links):\n",
    "                output_df = pd.DataFrame(results)\n",
    "                batch_number = batch_start_number + index // split_number\n",
    "                output_csv = os.path.join(\"Outputs\\\\Text_generation_results_2\", f'model_info_batch_{batch_number}.csv')\n",
    "                output_df.to_csv(output_csv, index=False)\n",
    "                print(f\"Finished processing batch {batch_number} and saved to {output_csv}\")\n",
    "                results.clear()\n",
    "\n",
    "# Start parallel processing\n",
    "process_links_in_parallel(nlp_links, username, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinopathy_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
