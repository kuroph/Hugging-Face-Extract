{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2-2b-it\n",
      "Finished processing batch 1 and saved to Outputs\\Text_generation_results_2\\model_info_batch_1.csv\n",
      "No arxiv tags for the link: https://huggingface.co/google/gemma-2-2b\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-405B\n",
      "Finished processing batch 2 and saved to Outputs\\Text_generation_results_2\\model_info_batch_2.csv\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
      "Finished processing batch 3 and saved to Outputs\\Text_generation_results_2\\model_info_batch_3.csv\n",
      "Finished processing batch 4 and saved to Outputs\\Text_generation_results_2\\model_info_batch_4.csv\n",
      "No arxiv tags for the link: https://huggingface.co/Writer/Palmyra-Med-70B-32K\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "Finished processing batch 5 and saved to Outputs\\Text_generation_results_2\\model_info_batch_5.csv\n",
      "Finished processing batch 6 and saved to Outputs\\Text_generation_results_2\\model_info_batch_6.csv\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "Finished processing batch 7 and saved to Outputs\\Text_generation_results_2\\model_info_batch_7.csv\n",
      "No arxiv tags for the link: https://huggingface.co/Writer/Palmyra-Med-70B\n",
      "Finished processing batch 8 and saved to Outputs\\Text_generation_results_2\\model_info_batch_8.csv\n",
      "No arxiv tags for the link: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B\n",
      "Finished processing batch 9 and saved to Outputs\\Text_generation_results_2\\model_info_batch_9.csv\n",
      "Unknown Error occured for the link https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scholarly import scholarly\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "username = os.getenv(\"HUGGINGFACE_USERNAME\")\n",
    "password = os.getenv(\"HUGGINGFACE_PASSWORD\")\n",
    "\n",
    "# Import files\n",
    "import Python_scripts.login as login\n",
    "import Python_scripts.extract_info as extract_info\n",
    "import Python_scripts.click_commit_dates as click_commit\n",
    "import Python_scripts.click_username as click_username\n",
    "import Python_scripts.click_arxiv_tags as click_arxiv_tags\n",
    "import Python_scripts.space_apps_info as space_apps_info\n",
    "import Python_scripts.get_submission_date as get_submission_date\n",
    "import Python_scripts.click_community as click_community\n",
    "import Python_scripts.check_404_error as check_404_error\n",
    "import Python_scripts.files_versions_info as files_versions_info\n",
    "importlib.reload(login)\n",
    "importlib.reload(extract_info)\n",
    "importlib.reload(click_commit)\n",
    "importlib.reload(click_username)\n",
    "importlib.reload(click_arxiv_tags)\n",
    "importlib.reload(space_apps_info)\n",
    "importlib.reload(get_submission_date)\n",
    "importlib.reload(click_community)\n",
    "importlib.reload(check_404_error)\n",
    "importlib.reload(files_versions_info)\n",
    "\n",
    "# Load links\n",
    "nlp_links = pd.read_csv(\"Outputs/model_links-NLP.csv\")\n",
    "nlp_links = nlp_links[nlp_links[\"Tag\"] == \"Text Generation\"]\n",
    "nlp_links.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "# Determine the starting batch number based on the slice\n",
    "split_number = 2\n",
    "batch_start_number = 1\n",
    "nlp_links = nlp_links[0:40]\n",
    "\n",
    "\n",
    "# Set up Chrome\n",
    "chrome_options = Options()\n",
    "# Display ON/OFF\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "service = Service(executable_path='Dependencies//chromedriver.exe') \n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "login.login_hugging_face(driver, username, password)\n",
    "\n",
    "results = []\n",
    "error_flag = 0\n",
    "\n",
    "\n",
    "# Loop for every link\n",
    "for index, row in nlp_links.iterrows():\n",
    "\n",
    "    downloads_all_time = has_arxiv = model_card = language_tag = model_name = likes_element = description_names = model_card_word_count = no_space_apps = 'NA'\n",
    "    community_count = discussions_count = discussions_closed_count = pull_request_count = pull_request_closed_count = 'NA'\n",
    "    info_text = name_text = 'NA'\n",
    "    number_of_commits = latest_commit_date = oldest_commit_date = additional_requirements = 'NA'\n",
    "    arxiv_links = number_of_papers = submission_dates_dt = 'NA'\n",
    "    published_dates_dt = readme_commit_dates = no_unique_readme_commits = time_differences = unique_commit_dates = no_unique_commits = 'NA'\n",
    "    error_flag = 'NA'\n",
    "\n",
    "    link = row['Model Link']\n",
    "\n",
    "    try:\n",
    "        error_flag = check_404_error.check_404_page(driver, link)\n",
    "        if error_flag == 0:\n",
    "            # Extract info from the link page\n",
    "            extract_info_list = extract_info.extract_info(driver, link)\n",
    "            downloads_per_month = extract_info_list[0]\n",
    "            downloads_all_time = extract_info_list[1]\n",
    "            model_card = extract_info_list[2]\n",
    "            language_tag = extract_info_list[3]\n",
    "            model_name = extract_info_list[4]\n",
    "            likes_element = extract_info_list[5]\n",
    "            description_names = extract_info_list[6]\n",
    "            model_card_word_count = extract_info_list[7]\n",
    "            no_space_apps =  extract_info_list[8]\n",
    "            \n",
    "            # Extract info after clicking the Community Tab\n",
    "            click_community_list = click_community.click_comm(driver, link)\n",
    "            community_count = click_community_list[0]\n",
    "            discussions_count = click_community_list[1]\n",
    "            discussions_closed_count = click_community_list[2]\n",
    "            pull_request_count = click_community_list[3]\n",
    "            pull_request_closed_count = click_community_list[4]\n",
    "\n",
    "            # Extract info after clicking on the username link\n",
    "            click_username_list = click_username.get_user_info(driver, link)\n",
    "            info_text = click_username_list[0]\n",
    "            name_text = click_username_list[1]\n",
    "            \n",
    "            # Extract info after clicking on the commits link\n",
    "            click_commit_list = click_commit.click_files_and_versions(driver, link)\n",
    "            number_of_commits = click_commit_list[0]\n",
    "            latest_commit_date = click_commit_list[1]\n",
    "            oldest_commit_date = click_commit_list[2]\n",
    "            additional_requirements = click_commit_list[3]\n",
    "            unique_commit_dates = click_commit_list[4]\n",
    "            no_unique_commits = len(unique_commit_dates)\n",
    "\n",
    "            # Extract info after clicking on the ARXIV tags\n",
    "            click_arxiv_tags_list = click_arxiv_tags.get_arxiv_links(driver, link)\n",
    "            arxiv_links = click_arxiv_tags_list[0]\n",
    "            number_of_papers = len(arxiv_links)\n",
    "            if number_of_papers == 0 or number_of_papers == 'NA':\n",
    "                has_arxiv = 0\n",
    "            elif number_of_papers > 0:\n",
    "                has_arxiv = 1\n",
    "            submission_dates = click_arxiv_tags_list[1]\n",
    "            submission_dates_dt = [datetime.strptime(date, '%d %b %Y') if date != \"N/A\" else \"N/A\" for date in submission_dates]\n",
    "\n",
    "            get_submission_date_list = get_submission_date.get_readme_info(driver, link)\n",
    "            arxiv_links_2 = get_submission_date_list[0]\n",
    "            published_dates = get_submission_date_list[1]\n",
    "            published_dates_dt = [datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') if date != \"N/A\" else \"N/A\" for date in published_dates]\n",
    "            readme_commit_dates = get_submission_date_list[2]\n",
    "            no_unique_readme_commits = len(readme_commit_dates)\n",
    "\n",
    "            time_differences = [\n",
    "                \"N/A\" if (isinstance(pub_date, str) or isinstance(sub_date, str) or pub_date is None or sub_date is None) \n",
    "                else (pub_date - sub_date).days\n",
    "                for pub_date, sub_date in zip(published_dates_dt, submission_dates_dt)\n",
    "            ]\n",
    "\n",
    "    except:\n",
    "        print(f\"Unknown Error occured for the link {link}\")\n",
    "\n",
    "    \n",
    "    results.append({\n",
    "        'Model Link': link,\n",
    "        'Valid Link?' : error_flag,\n",
    "        'Model Name' : model_name,\n",
    "        'Language of the Model' : language_tag,\n",
    "        'Organization Tags' : info_text,\n",
    "        'Name of Organization/Individual' : name_text,\n",
    "        'Downloads All Time': downloads_all_time,\n",
    "        'Likes' : likes_element,\n",
    "        'Community Contributions' : community_count,\n",
    "        'Number of Discussions' : discussions_count,\n",
    "        'Closed Discussions' : discussions_closed_count,\n",
    "        'Number of Pull Requests' : pull_request_count,\n",
    "        'Closed Pull Requests' : pull_request_closed_count,\n",
    "        'Has Arxiv Tag': has_arxiv,\n",
    "        'Number of Papers' : number_of_papers,\n",
    "        'Links to Paper(s)' : arxiv_links,\n",
    "        'Publish Dates of Paper(s)' : submission_dates_dt,\n",
    "        'Publish Dates of Paper(s) (on HF)' : published_dates_dt,\n",
    "        'Time Difference' : time_differences,\n",
    "        'Model Card': model_card,\n",
    "        'Model Card Length' : model_card_word_count,\n",
    "        'Number of Sections' : len(description_names),\n",
    "        'Model Card Section Names' : description_names,\n",
    "        'Number of Commits': number_of_commits,\n",
    "        'Latest Commit Date': latest_commit_date,\n",
    "        'Oldest Commit Date': oldest_commit_date,\n",
    "        'Unique Readme Commit Dates' : readme_commit_dates,\n",
    "        'Number of Unique Readme Commit Dates' : no_unique_readme_commits,\n",
    "        'Unique Commit Dates' : unique_commit_dates,\n",
    "        'Number of Unique Commit Dates' : no_unique_commits,\n",
    "        'Number of Space Apps' : no_space_apps,\n",
    "        'Additional Requirements' : additional_requirements\n",
    "    })\n",
    "    \n",
    "    # Save to CSV every split_number links processed\n",
    "    if (index + 1) % split_number == 0 or (index + 1) == len(nlp_links):\n",
    "        output_df = pd.DataFrame(results)\n",
    "        batch_number = batch_start_number + index // split_number\n",
    "        output_csv = os.path.join(\"Outputs\\Text_generation_results_2\", f'model_info_batch_{batch_number}.csv')\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "        \n",
    "        print(f\"Finished processing batch {batch_number} and saved to {output_csv}\")\n",
    "\n",
    "        # Clear results list to start next batch\n",
    "        results.clear()\n",
    "\n",
    "# Close the driver after completion\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n",
      "2500\n",
      "2550\n",
      "Error navigating to 'Community' tab for https://huggingface.co/samaysk/megaspringllamaft\n",
      "2600\n",
      "No arxiv tags for the link: https://huggingface.co/Flemem/TunedLlama-7B\n",
      "2650\n",
      "Error navigating to 'Community' tab for https://huggingface.co/sequelbox/Llama2-13B-DaringFortitude\n",
      "No arxiv tags for the link: https://huggingface.co/Trelis/falcon-40B-chat-SFT\n",
      "2700\n",
      "Error navigating to 'Community' tab for https://huggingface.co/RUCAIBox/rear-llama-7b-hf\n",
      "No arxiv tags for the link: https://huggingface.co/ethz-spylab/poisoned_generation_trojan5\n",
      "Error navigating to 'Community' tab for https://huggingface.co/CausalLM/7B\n",
      "Error navigating to 'Community' tab for https://huggingface.co/solakim/Doug-Mistral-Chat\n",
      "No arxiv tags for the link: https://huggingface.co/solakim/Doug-Mistral-Chat\n",
      "2750\n",
      "Error navigating to 'Community' tab for https://huggingface.co/ParasiticRogue/Nyakura-CausalLM-RP-34B\n",
      "2800\n",
      "Error navigating to 'Community' tab for https://huggingface.co/ParasiticRogue/Nontoxic-PiVoT-Bagel-RP-34b\n",
      "No arxiv tags for the link: https://huggingface.co/Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\n",
      "2850\n",
      "Error navigating to 'Community' tab for https://huggingface.co/pp3232133/distilgpt2-wikitext2\n",
      "No arxiv tags for the link: https://huggingface.co/thusinh1969/LLaMA-2-Instruct-Chat-100k-08MMAR2024\n",
      "2900\n",
      "No arxiv tags for the link: https://huggingface.co/TRAC-MTRY/traclm-v2-7b-instruct-GPTQ\n",
      "No arxiv tags for the link: https://huggingface.co/jefferylovely/MoeLovely-13B\n",
      "2950\n",
      "Unknown Error occured for the link https://huggingface.co/jaehy12/gemma-dinolabs-2b-ko\n",
      "Unknown Error occured for the link https://huggingface.co/CognitoLibera/net0\n",
      "Unknown Error occured for the link https://huggingface.co/icefog72/IceTeaRP-7b-8.0bpw-exl2\n",
      "Unknown Error occured for the link https://huggingface.co/han2lin/gpt2_med_s23_ft_small\n",
      "3000\n",
      "No arxiv tags for the link: https://huggingface.co/leoleoasd/smol-code-llama\n",
      "3050\n",
      "No arxiv tags for the link: https://huggingface.co/TRAC-MTRY/traclm-v2-7b-instruct-AWQ\n",
      "3100\n",
      "No arxiv tags for the link: https://huggingface.co/Sahi19/Gemma2bLegalChatbot\n",
      "Error navigating to 'Community' tab for https://huggingface.co/cognisys/sparrow-1.1b-chat-alpha\n",
      "3150\n",
      "No arxiv tags for the link: https://huggingface.co/coastalcph/Llama-2-13b-chat-hf-LoRA-eu-debates-id\n",
      "Error navigating to 'Community' tab for https://huggingface.co/solakim/Doug-Mistral-V2\n",
      "No arxiv tags for the link: https://huggingface.co/solakim/Doug-Mistral-V2\n",
      "Error navigating to 'Community' tab for https://huggingface.co/nuprl/MultiPL-T-StarCoder2_15B\n",
      "Error navigating to 'Community' tab for https://huggingface.co/solakim/Sierra-Mistral-V2\n",
      "No arxiv tags for the link: https://huggingface.co/solakim/Sierra-Mistral-V2\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scholarly import scholarly\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "username = os.getenv(\"HUGGINGFACE_USERNAME\")\n",
    "password = os.getenv(\"HUGGINGFACE_PASSWORD\")\n",
    "\n",
    "# Import files\n",
    "import Python_scripts.login as login\n",
    "import Python_scripts.extract_info as extract_info\n",
    "import Python_scripts.click_commit_dates as click_commit\n",
    "import Python_scripts.click_username as click_username\n",
    "import Python_scripts.click_arxiv_tags as click_arxiv_tags\n",
    "import Python_scripts.space_apps_info as space_apps_info\n",
    "import Python_scripts.get_submission_date as get_submission_date\n",
    "import Python_scripts.click_community as click_community\n",
    "import Python_scripts.check_404_error as check_404_error\n",
    "import Python_scripts.files_versions_info as files_versions_info\n",
    "importlib.reload(login)\n",
    "importlib.reload(extract_info)\n",
    "importlib.reload(click_commit)\n",
    "importlib.reload(click_username)\n",
    "importlib.reload(click_arxiv_tags)\n",
    "importlib.reload(space_apps_info)\n",
    "importlib.reload(get_submission_date)\n",
    "importlib.reload(click_community)\n",
    "importlib.reload(check_404_error)\n",
    "importlib.reload(files_versions_info)\n",
    "\n",
    "# Load links\n",
    "#nlp_links = pd.read_csv(\"Outputs/model_links-NLP.csv\")\n",
    "nlp_links = pd.read_csv(\"Outputs/Errors_retry.csv\")\n",
    "nlp_links = nlp_links[nlp_links[\"Tag\"] == \"Text Generation\"]\n",
    "nlp_links.reset_index(inplace = True, drop = True)\n",
    "nlp_links = nlp_links[2500:]\n",
    "\n",
    "# Determine the starting batch number based on the slice\n",
    "split_number = 2\n",
    "batch_start_number = 1\n",
    "\n",
    "\n",
    "# Set up Chrome\n",
    "chrome_options = Options()\n",
    "# Display ON/OFF\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "service = Service(executable_path='Dependencies//chromedriver.exe') \n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "login.login_hugging_face(driver, username, password)\n",
    "\n",
    "results = []\n",
    "error_flag = 0\n",
    "\n",
    "\n",
    "# Loop for every link\n",
    "for index, row in nlp_links.iterrows():\n",
    "\n",
    "    downloads_all_time = has_arxiv = model_card = language_tag = model_name = likes_element = description_names = model_card_word_count = no_space_apps = 'NA'\n",
    "    community_count = discussions_count = discussions_closed_count = pull_request_count = pull_request_closed_count = 'NA'\n",
    "    info_text = name_text = 'NA'\n",
    "    number_of_commits = latest_commit_date = oldest_commit_date = additional_requirements = 'NA'\n",
    "    arxiv_links = number_of_papers = submission_dates_dt = 'NA'\n",
    "    published_dates_dt = readme_commit_dates = no_unique_readme_commits = time_differences = unique_commit_dates = no_unique_commits = 'NA'\n",
    "    error_flag = 'NA'\n",
    "\n",
    "    link = row['Model Link']\n",
    "    if(index%50 == 0):\n",
    "        print(index)\n",
    "\n",
    "\n",
    "    try:\n",
    "        error_flag = check_404_error.check_404_page(driver, link)\n",
    "        if error_flag == 0:\n",
    "            # Extract info from the link page\n",
    "            extract_info_list = extract_info.extract_info(driver, link)\n",
    "            downloads_per_month = extract_info_list[0]\n",
    "            downloads_all_time = extract_info_list[1]\n",
    "            model_card = extract_info_list[2]\n",
    "            language_tag = extract_info_list[3]\n",
    "            model_name = extract_info_list[4]\n",
    "            likes_element = extract_info_list[5]\n",
    "            description_names = extract_info_list[6]\n",
    "            model_card_word_count = extract_info_list[7]\n",
    "            no_space_apps =  extract_info_list[8]\n",
    "            \n",
    "            # Extract info after clicking the Community Tab\n",
    "            click_community_list = click_community.click_comm(driver, link)\n",
    "            community_count = click_community_list[0]\n",
    "            discussions_count = click_community_list[1]\n",
    "            discussions_closed_count = click_community_list[2]\n",
    "            pull_request_count = click_community_list[3]\n",
    "            pull_request_closed_count = click_community_list[4]\n",
    "\n",
    "            # Extract info after clicking on the username link\n",
    "            click_username_list = click_username.get_user_info(driver, link)\n",
    "            info_text = click_username_list[0]\n",
    "            name_text = click_username_list[1]\n",
    "            \n",
    "            # Extract info after clicking on the commits link\n",
    "            click_commit_list = click_commit.click_files_and_versions(driver, link)\n",
    "            number_of_commits = click_commit_list[0]\n",
    "            latest_commit_date = click_commit_list[1]\n",
    "            oldest_commit_date = click_commit_list[2]\n",
    "            additional_requirements = click_commit_list[3]\n",
    "            unique_commit_dates = click_commit_list[4]\n",
    "            no_unique_commits = len(unique_commit_dates)\n",
    "\n",
    "            # Extract info after clicking on the ARXIV tags\n",
    "            click_arxiv_tags_list = click_arxiv_tags.get_arxiv_links(driver, link)\n",
    "            arxiv_links = click_arxiv_tags_list[0]\n",
    "            number_of_papers = len(arxiv_links)\n",
    "            if number_of_papers == 0 or number_of_papers == 'NA':\n",
    "                has_arxiv = 0\n",
    "            elif number_of_papers > 0:\n",
    "                has_arxiv = 1\n",
    "            submission_dates = click_arxiv_tags_list[1]\n",
    "            submission_dates_dt = [datetime.strptime(date, '%d %b %Y') if date != \"N/A\" else \"N/A\" for date in submission_dates]\n",
    "\n",
    "            get_submission_date_list = get_submission_date.get_readme_info(driver, link)\n",
    "            arxiv_links_2 = get_submission_date_list[0]\n",
    "            published_dates = get_submission_date_list[1]\n",
    "            published_dates_dt = [datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') if date != \"N/A\" else \"N/A\" for date in published_dates]\n",
    "            readme_commit_dates = get_submission_date_list[2]\n",
    "            # no_unique_readme_commits = len(readme_commit_dates)\n",
    "\n",
    "            time_differences = [\n",
    "                \"N/A\" if (isinstance(pub_date, str) or isinstance(sub_date, str) or pub_date is None or sub_date is None) \n",
    "                else (pub_date - sub_date).days\n",
    "                for pub_date, sub_date in zip(published_dates_dt, submission_dates_dt)\n",
    "            ]\n",
    "\n",
    "    except:\n",
    "        print(f\"Unknown Error occured for the link {link}\")\n",
    "\n",
    "    \n",
    "    results.append({\n",
    "        'Model Link': link,\n",
    "        'Valid Link?' : error_flag,\n",
    "        'Model Name' : model_name,\n",
    "        'Language of the Model' : language_tag,\n",
    "        'Organization Tags' : info_text,\n",
    "        'Name of Organization/Individual' : name_text,\n",
    "        'Downloads All Time': downloads_all_time,\n",
    "        'Likes' : likes_element,\n",
    "        'Community Contributions' : community_count,\n",
    "        'Number of Discussions' : discussions_count,\n",
    "        'Closed Discussions' : discussions_closed_count,\n",
    "        'Number of Pull Requests' : pull_request_count,\n",
    "        'Closed Pull Requests' : pull_request_closed_count,\n",
    "        'Has Arxiv Tag': has_arxiv,\n",
    "        'Number of Papers' : number_of_papers,\n",
    "        'Links to Paper(s)' : arxiv_links,\n",
    "        'Publish Dates of Paper(s)' : submission_dates_dt,\n",
    "        'Publish Dates of Paper(s) (on HF)' : published_dates_dt,\n",
    "        'Time Difference' : time_differences,\n",
    "        'Model Card': model_card,\n",
    "        'Model Card Length' : model_card_word_count,\n",
    "        'Number of Sections' : len(description_names),\n",
    "        'Model Card Section Names' : description_names,\n",
    "        'Number of Commits': number_of_commits,\n",
    "        'Latest Commit Date': latest_commit_date,\n",
    "        'Oldest Commit Date': oldest_commit_date,\n",
    "        'Unique Readme Commit Dates' : readme_commit_dates,\n",
    "        'Number of Unique Readme Commit Dates' : no_unique_readme_commits,\n",
    "        'Unique Commit Dates' : unique_commit_dates,\n",
    "        'Number of Unique Commit Dates' : no_unique_commits,\n",
    "        'Number of Space Apps' : no_space_apps,\n",
    "        'Additional Requirements' : additional_requirements\n",
    "    })\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(results)\n",
    "output_csv = os.path.join(\"Outputs\", f'Errors_retry_results_10.csv')\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "# Close the driver after completion\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scholarly import scholarly\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "username = os.getenv(\"HUGGINGFACE_USERNAME\")\n",
    "password = os.getenv(\"HUGGINGFACE_PASSWORD\")\n",
    "\n",
    "# Import files\n",
    "import Python_scripts.login as login\n",
    "import Python_scripts.extract_info as extract_info\n",
    "import Python_scripts.click_commit_dates as click_commit\n",
    "import Python_scripts.click_username as click_username\n",
    "import Python_scripts.click_arxiv_tags as click_arxiv_tags\n",
    "import Python_scripts.space_apps_info as space_apps_info\n",
    "import Python_scripts.get_submission_date as get_submission_date\n",
    "import Python_scripts.click_community as click_community\n",
    "import Python_scripts.check_404_error as check_404_error\n",
    "import Python_scripts.files_versions_info as files_versions_info\n",
    "importlib.reload(login)\n",
    "importlib.reload(extract_info)\n",
    "importlib.reload(click_commit)\n",
    "importlib.reload(click_username)\n",
    "importlib.reload(click_arxiv_tags)\n",
    "importlib.reload(space_apps_info)\n",
    "importlib.reload(get_submission_date)\n",
    "importlib.reload(click_community)\n",
    "importlib.reload(check_404_error)\n",
    "importlib.reload(files_versions_info)\n",
    "\n",
    "# Determine the starting batch number based on the slice\n",
    "split_number = 2\n",
    "batch_start_number = 1\n",
    "\n",
    "\n",
    "# Set up Chrome\n",
    "chrome_options = Options()\n",
    "# Display ON/OFF\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "service = Service(executable_path='Dependencies//chromedriver.exe') \n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "login.login_hugging_face(driver, username, password)\n",
    "\n",
    "results = []\n",
    "error_flag = 0\n",
    "\n",
    "\n",
    "links = [\"https://huggingface.co/bartowski/Aethora-7b-v1-exl2\",\n",
    "         \"https://huggingface.co/cgus/LLaMA2-13B-Erebus-v3-exl2\"]\n",
    "\n",
    "\n",
    "# Loop for every link\n",
    "for index, link in enumerate(links):\n",
    "\n",
    "    downloads_all_time = has_arxiv = model_card = language_tag = model_name = likes_element = description_names = model_card_word_count = no_space_apps = 'NA'\n",
    "    community_count = discussions_count = discussions_closed_count = pull_request_count = pull_request_closed_count = 'NA'\n",
    "    info_text = name_text = 'NA'\n",
    "    number_of_commits = latest_commit_date = oldest_commit_date = additional_requirements = 'NA'\n",
    "    arxiv_links = number_of_papers = submission_dates_dt = 'NA'\n",
    "    published_dates_dt = readme_commit_dates = no_unique_readme_commits = time_differences = unique_commit_dates = no_unique_commits = 'NA'\n",
    "    error_flag = 'NA'\n",
    "\n",
    "    if(index%50 == 0):\n",
    "        print(index)\n",
    "\n",
    "\n",
    "    error_flag = check_404_error.check_404_page(driver, link)\n",
    "    if error_flag == 0:\n",
    "        # Extract info from the link page\n",
    "        extract_info_list = extract_info.extract_info(driver, link)\n",
    "        downloads_per_month = extract_info_list[0]\n",
    "        downloads_all_time = extract_info_list[1]\n",
    "        has_arxiv = extract_info_list[2] ######\n",
    "        model_card = extract_info_list[3]\n",
    "        language_tag = extract_info_list[4] ######\n",
    "        model_name = extract_info_list[5]\n",
    "        likes_element = extract_info_list[6]\n",
    "        description_names = extract_info_list[7]\n",
    "        model_card_word_count = extract_info_list[8]\n",
    "        no_space_apps =  extract_info_list[9]\n",
    "\n",
    "        # Extract info after clicking the Community Tab\n",
    "        click_community_list = click_community.click_comm(driver, link)\n",
    "        community_count = click_community_list[0]\n",
    "        discussions_count = click_community_list[1]\n",
    "        discussions_closed_count = click_community_list[2]\n",
    "        pull_request_count = click_community_list[3]\n",
    "        pull_request_closed_count = click_community_list[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinopathy_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
